{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC0j66cDBBvx",
        "outputId": "04cec599-dd06-4852-a794-915287725e0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '11785-TCR'...\n",
            "remote: Enumerating objects: 1135, done.\u001b[K\n",
            "remote: Counting objects: 100% (345/345), done.\u001b[K\n",
            "remote: Compressing objects: 100% (174/174), done.\u001b[K\n",
            "remote: Total 1135 (delta 204), reused 298 (delta 157), pack-reused 790 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1135/1135), 4.70 MiB | 13.57 MiB/s, done.\n",
            "Resolving deltas: 100% (640/640), done.\n",
            "Transformed train file saved at: /content/train_PyTDC_to_AVIB.csv\n",
            "Transformed test file saved at: /content/test_PyTDC_to_AVIB.csv\n",
            "Transformed Train Dataset Preview:\n",
            "          tcra                 tcrb      peptide  sign\n",
            "0      unknown       CSVWGTGKTYEQYF     FLKEKGGL     1\n",
            "1      unknown     CSATILAGVPYGEQYF     FLKEKGGL     1\n",
            "2      unknown       CSASEGTSSYEQYF     FLKEKGGL     1\n",
            "3      unknown  CAISESGYGGPPGANVLTF     FLKEKGGL     1\n",
            "4      unknown  CAISEPGYRGPPGANVLTF     FLKEKGGL     1\n",
            "...        ...                  ...          ...   ...\n",
            "42459  unknown       CASSLEWGGETQYF    RLDKVEAEV     0\n",
            "42460  unknown         CASSAREGKLFF    FLPRVFSAV     0\n",
            "42461  unknown       CASRPSGGAETQYF  HPVGEADYFEY     0\n",
            "42462  unknown         CASSRHTGELFF   KRWIILGLNK     1\n",
            "42463  unknown         CASSLGTTEAFF    RLDKVEAEV     1\n",
            "\n",
            "[42464 rows x 4 columns]\n",
            "Transformed Test Dataset Preview:\n",
            "         tcra               tcrb     peptide  sign\n",
            "0     unknown  CASSEWGMDGTTDTQYF   TLIGDCATV     1\n",
            "1     unknown       CASGQDTGELFF   FIAGLIAIV     1\n",
            "2     unknown     CSVSGNPSTGELFF   KLSYGIATV     1\n",
            "3     unknown  CASSFHSGVPMGETQYF   ALSKGVHFV     1\n",
            "4     unknown  CASSASSVQLLGDTQYF   RLRAEAQVK     1\n",
            "...       ...                ...         ...   ...\n",
            "4713  unknown     CASSIGQGARGYTF  ELAGIGILTV     1\n",
            "4714  unknown   CASSDRGGRNTDTQYF   LVLSVNPYV     0\n",
            "4715  unknown    CASSYGQGPAGEAFF   IQYIDIGNY     1\n",
            "4716  unknown    CASSDREVDYNEQFF   KLWAQCVQL     1\n",
            "4717  unknown        CASSEDAGYTF   KLWAQCVQL     1\n",
            "\n",
            "[4718 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "# train_PyTDC_new -> 42464 rows = 33028 rows + 9436 rows\n",
        "# test_PyTDC_new -> 4718 rows\n",
        "\n",
        "# PyTDC Rows -> epitope_aa, epitope_smi, tcr, tcr_full, label\n",
        "# TLIGDCATV,\n",
        "# CC[C@H](C)[C@H](NC(=O)[C@H](CC(C)C)NC(=O)[C@@H](N)[C@@H](C)O)C(=O)NCC(=O)N[C@@H](CC(=O)O)C(=O)N[C@@H](CS)C(=O)N[C@@H](C)C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C(C)C)[C@@H](C)O,\n",
        "# CASSEWGMDGTTDTQYF,\n",
        "# NAGVTQTPKFQVLKTGQSMTLQCAQDMNHNSMYWYRQDPGMGLRLIYYSASEGTTDKGEVPNGYNVSRLNKREFSLRLESAAPSQTSVYFCASSEWGMDGTTDTQYFGPGTRLTVL,\n",
        "# 1\n",
        "\n",
        "# AVIB Beta Rows -> tcra, tcrb, peptide, sign\n",
        "# unknown\n",
        "# CASRECIATWHHQPQHF\n",
        "# SSLENFRAYV\n",
        "# 0-1\n",
        "\n",
        "# tcra to unkown\n",
        "# our tcr -> tcrb\n",
        "# our epitope_aa -> peptide\n",
        "# our label -> sign\n",
        "\n",
        "# Step 1: Install necessary libraries (if not already installed)\n",
        "import pandas as pd\n",
        "\n",
        "# Step 2: Clone the GitHub repository to access the dataset\n",
        "!git clone https://github.com/Yichuan0712/11785-TCR.git\n",
        "\n",
        "# Step 3: Set the file paths\n",
        "base_path = \"/content/11785-TCR/dataset/pytdc_new/\"\n",
        "train1_file = base_path + \"train1_PyTDC.csv\"\n",
        "train2_file = base_path + \"train2_PyTDC.csv\"\n",
        "test_file = base_path + \"test_PyTDC.csv\"\n",
        "\n",
        "# Step 4: Load the CSV files\n",
        "train1_df = pd.read_csv(train1_file)\n",
        "train2_df = pd.read_csv(train2_file)\n",
        "test_df = pd.read_csv(test_file)\n",
        "\n",
        "# Step 5: Merge the train datasets\n",
        "merged_train_df = pd.concat([train1_df, train2_df], ignore_index=True)\n",
        "\n",
        "# Step 6: Create transformation function for AVIB Beta format\n",
        "def transform_to_avib_format(df):\n",
        "    avib_df = pd.DataFrame({\n",
        "        \"tcra\": \"unknown\",  # Set the 'tcra' column to 'unknown'\n",
        "        \"tcrb\": df[\"tcr\"],  # Map 'tcr' to 'tcrb'\n",
        "        \"peptide\": df[\"epitope_aa\"],  # Map 'epitope_aa' to 'peptide'\n",
        "        \"sign\": df[\"label\"],  # Map 'label' to 'sign'\n",
        "    })\n",
        "    return avib_df\n",
        "\n",
        "# Step 7: Transform train and test datasets to AVIB Beta format\n",
        "train_avib_df = transform_to_avib_format(merged_train_df)\n",
        "test_avib_df = transform_to_avib_format(test_df)\n",
        "\n",
        "# Step 8: Save the transformed datasets to CSV files\n",
        "train_avib_path = \"/content/train_PyTDC_to_AVIB.csv\"\n",
        "test_avib_path = \"/content/test_PyTDC_to_AVIB.csv\"\n",
        "train_avib_df.to_csv(train_avib_path, index=False)\n",
        "test_avib_df.to_csv(test_avib_path, index=False)\n",
        "\n",
        "print(f\"Transformed train file saved at: {train_avib_path}\")\n",
        "print(f\"Transformed test file saved at: {test_avib_path}\")\n",
        "\n",
        "# Step 9: Verify the transformed files\n",
        "print(\"Transformed Train Dataset Preview:\")\n",
        "print(train_avib_df)\n",
        "\n",
        "print(\"Transformed Test Dataset Preview:\")\n",
        "print(test_avib_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attentive Variational Information Bottleneck.\n",
        "\n",
        "In this notebook, we train and test the Attentive Variational Information Bottleneck (MVIB [1] with Attention of Experts) and MVIB on all datasets."
      ],
      "metadata": {
        "id": "a1BOBdXgZU9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/nec-research/vibtcr/blob/07169bf6a0a0f1620b56f9d744d0e8bfee50d38d/notebooks/notebooks.classification/avib.ipynb#L48\n",
        "\n",
        "!git clone https://github.com/nec-research/vibtcr.git\n",
        "# import sys\n",
        "# sys.path.append('/content/vibtcr/vibtcr')\n",
        "import os\n",
        "os.chdir('/content/vibtcr/vibtcr')\n",
        "current_path = os.getcwd()\n",
        "os.listdir(current_path)\n",
        "\n",
        "# !pip install numpy pandas scikit-learn torch torchvision torchaudio tqdm\n",
        "\n",
        "# !pip install numpy==1.19.4\n",
        "# !pip install pandas==1.1.4\n",
        "# !pip install scikit-learn==0.24.2\n",
        "# !pip install torch==1.10.0 torchvision==0.11.1 torchaudio==0.10.0 tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzDnPBqMqz5F",
        "outputId": "a1236389-a271-441d-b2be-450c4801df93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vibtcr'...\n",
            "remote: Enumerating objects: 219, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 219 (delta 19), reused 13 (delta 4), pack-reused 180 (from 1)\u001b[K\n",
            "Receiving objects: 100% (219/219), 44.30 MiB | 21.47 MiB/s, done.\n",
            "Resolving deltas: 100% (93/93), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vibtcr', 'requirements.txt', 'setup.py']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, accuracy_score, precision_score, recall_score, f1_score,\n",
        "    precision_recall_curve, auc\n",
        ")\n",
        "from vibtcr.dataset import TCRDataset\n",
        "from vibtcr.mvib.mvib import MVIB\n",
        "from vibtcr.mvib.mvib_trainer import TrainerMVIB\n",
        "\n",
        "# Step 1: Define helper functions\n",
        "metrics = [\n",
        "    'auROC',\n",
        "    'Accuracy',\n",
        "    'Recall',\n",
        "    'Precision',\n",
        "    'F1 score',\n",
        "    'auPRC'\n",
        "]\n",
        "\n",
        "def pr_auc(y_true, y_prob):\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    return pr_auc\n",
        "\n",
        "def get_scores(y_true, y_prob, y_pred):\n",
        "    \"\"\"\n",
        "    Compute a df with all classification metrics and respective scores.\n",
        "    \"\"\"\n",
        "    scores = [\n",
        "        roc_auc_score(y_true, y_prob),\n",
        "        accuracy_score(y_true, y_pred),\n",
        "        recall_score(y_true, y_pred),\n",
        "        precision_score(y_true, y_pred),\n",
        "        f1_score(y_true, y_pred),\n",
        "        pr_auc(y_true, y_prob)\n",
        "    ]\n",
        "    df = pd.DataFrame(data={'score': scores, 'metrics': metrics})\n",
        "    return df\n",
        "\n",
        "def set_random_seed(random_seed):\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "# Step 2: Model and Training Configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "batch_size = 4096\n",
        "epochs = 500\n",
        "lr = 1e-3\n",
        "z_dim = 150\n",
        "beta = 1e-6\n",
        "early_stopper_patience = 50\n",
        "monitor = 'auROC'\n",
        "lr_scheduler_param = 10\n",
        "joint_posterior = \"aoe\"\n",
        "results_base = \"/content/\""
      ],
      "metadata": {
        "id": "xXqqeY1TZBdQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t7LdyHZIZLuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: File paths\n",
        "train_path = \"/content/train_PyTDC_to_AVIB.csv\"\n",
        "test_path = \"/content/test_PyTDC_to_AVIB.csv\"\n",
        "\n",
        "# Step 4: Load data\n",
        "df_train_full = pd.read_csv(train_path)\n",
        "df_test_full = pd.read_csv(test_path)\n",
        "\n",
        "# Step 5: Train and Evaluate\n",
        "for i in range(5):  # 5 independent train/test splits\n",
        "    set_random_seed(i)\n",
        "\n",
        "    # Split training and validation sets\n",
        "    df_train, df_val = train_test_split(df_train_full, test_size=0.2, stratify=df_train_full.sign, random_state=i)\n",
        "\n",
        "    # Create datasets and scalers\n",
        "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
        "    ds_test = TCRDataset(df_test_full, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
        "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
        "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
        "\n",
        "    # Train loader with balanced sampling\n",
        "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
        "    weight = 1. / class_count\n",
        "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, sampler=sampler)\n",
        "\n",
        "    # Validation loader with balanced sampling\n",
        "    class_count_val = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
        "    weight_val = 1. / class_count_val\n",
        "    samples_weight_val = torch.tensor([weight_val[s] for s in df_val.sign])\n",
        "    val_sampler = WeightedRandomSampler(samples_weight_val, len(samples_weight_val))\n",
        "    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "    # Initialize model and trainer\n",
        "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
        "    trainer = TrainerMVIB(\n",
        "        model,\n",
        "        epochs=epochs,\n",
        "        lr=lr,\n",
        "        beta=beta,\n",
        "        checkpoint_dir=\".\",\n",
        "        mode=\"bimodal\",\n",
        "        lr_scheduler_param=lr_scheduler_param\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)\n",
        "\n",
        "    # Test the model\n",
        "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
        "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
        "    pred = pred.detach().numpy()\n",
        "    df_test_full['prediction_' + str(i)] = pred.squeeze().tolist()\n",
        "\n",
        "    # Save results\n",
        "    output_file = results_base + f\"mvib_bimodal_{joint_posterior}_split_{i}.csv\"\n",
        "    df_test_full.to_csv(output_file, index=False)\n",
        "    print(f\"Results saved at: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpOj-GalZL-O",
        "outputId": "40e26d41-b2c5-41b3-a9e0-3b52dff18707"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[VAL] Best epoch 16 | Best val score -0.760658 | DKL-prior 0.000219 | BCE 0.582420 | auROC 0.7607:  13%|█▎        | 65/500 [01:52<12:33,  1.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved at: /content/mvib_bimodal_aoe_split_0.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[VAL] Best epoch 16 | Best val score -0.756093 | DKL-prior 0.000256 | BCE 0.585861 | auROC 0.7561:  13%|█▎        | 65/500 [01:51<12:26,  1.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved at: /content/mvib_bimodal_aoe_split_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[VAL] Best epoch 40 | Best val score -0.762572 | DKL-prior 0.000265 | BCE 0.583028 | auROC 0.7626:  18%|█▊        | 89/500 [02:32<11:42,  1.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved at: /content/mvib_bimodal_aoe_split_2.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[VAL] Best epoch 8 | Best val score -0.752527 | DKL-prior 0.000225 | BCE 0.589948 | auROC 0.7525:  11%|█▏        | 57/500 [01:38<12:45,  1.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved at: /content/mvib_bimodal_aoe_split_3.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[VAL] Best epoch 38 | Best val score -0.772351 | DKL-prior 0.000271 | BCE 0.579199 | auROC 0.7724:  17%|█▋        | 87/500 [02:28<11:46,  1.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved at: /content/mvib_bimodal_aoe_split_4.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a new CSV file to have the mean prediction value\n",
        "file_path = \"/content/mvib_bimodal_aoe_split_4.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "df['prediction_mean'] = df[[f'prediction_{i}' for i in range(5)]].mean(axis=1)\n",
        "output_file = \"/content/mvib_bimodal_aoe_mean.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5zdJW5G1gaO",
        "outputId": "b5e7dfe2-f948-4d15-d643-32c1fed18c3d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         tcra               tcrb     peptide  sign  prediction_0  \\\n",
            "0     unknown  CASSEWGMDGTTDTQYF   TLIGDCATV     1      0.601226   \n",
            "1     unknown       CASGQDTGELFF   FIAGLIAIV     1      0.548987   \n",
            "2     unknown     CSVSGNPSTGELFF   KLSYGIATV     1      0.873805   \n",
            "3     unknown  CASSFHSGVPMGETQYF   ALSKGVHFV     1      0.619617   \n",
            "4     unknown  CASSASSVQLLGDTQYF   RLRAEAQVK     1      0.596857   \n",
            "...       ...                ...         ...   ...           ...   \n",
            "4713  unknown     CASSIGQGARGYTF  ELAGIGILTV     1      0.876717   \n",
            "4714  unknown   CASSDRGGRNTDTQYF   LVLSVNPYV     0      0.434205   \n",
            "4715  unknown    CASSYGQGPAGEAFF   IQYIDIGNY     1      0.576165   \n",
            "4716  unknown    CASSDREVDYNEQFF   KLWAQCVQL     1      0.763381   \n",
            "4717  unknown        CASSEDAGYTF   KLWAQCVQL     1      0.736568   \n",
            "\n",
            "      prediction_1  prediction_2  prediction_3  prediction_4  prediction_mean  \n",
            "0         0.838134      0.650785      0.462484      0.645073         0.639541  \n",
            "1         0.554488      0.667171      0.866325      0.652970         0.657988  \n",
            "2         0.508840      0.341995      0.746579      0.795152         0.653274  \n",
            "3         0.726247      0.503572      0.466865      0.700363         0.603333  \n",
            "4         0.693847      0.443795      0.537534      0.767554         0.607917  \n",
            "...            ...           ...           ...           ...              ...  \n",
            "4713      0.676959      0.828658      0.869123      0.759568         0.802205  \n",
            "4714      0.627881      0.352882      0.589590      0.259395         0.452791  \n",
            "4715      0.531643      0.417891      0.402082      0.467798         0.479116  \n",
            "4716      0.745755      0.725353      0.859210      0.796461         0.778032  \n",
            "4717      0.787954      0.565852      0.823704      0.827493         0.748314  \n",
            "\n",
            "[4718 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reference code\n",
        "# https://github.com/Yichuan0712/11785-TCR/blob/main/xgb.py\n",
        "\n",
        "# !pip install -q condacolab\n",
        "# import condacolab\n",
        "# condacolab.install()\n",
        "# !conda install -c conda-forge rdkit -y\n",
        "\n",
        "!pip install rdkit-pypi\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, average_precision_score, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "\n",
        "# Function to extract features from SMILES strings\n",
        "def extract_features(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return pd.Series([None] * 5)\n",
        "    features = {\n",
        "        'mol_weight': Descriptors.MolWt(mol),\n",
        "        'logP': Descriptors.MolLogP(mol),\n",
        "        'tpsa': Descriptors.TPSA(mol),\n",
        "        'num_h_donors': Descriptors.NumHDonors(mol),\n",
        "        'num_h_acceptors': Descriptors.NumHAcceptors(mol)\n",
        "    }\n",
        "    return pd.Series(features)\n",
        "\n",
        "# Function to prepare features for the dataset\n",
        "def prepare_features(df, use_smi=False):\n",
        "    if use_smi and 'epitope_smi' in df.columns:\n",
        "        smiles_features = df['epitope_smi'].apply(extract_features)\n",
        "        df = pd.concat([df, smiles_features], axis=1)\n",
        "        df = df.drop(columns=['epitope_smi'])\n",
        "    return df\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/mvib_bimodal_aoe_mean.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Split features and labels\n",
        "X = df[['tcrb', 'peptide', 'prediction_0']]\n",
        "y_true = df['sign']\n",
        "\n",
        "# Prepare features (you can expand this based on additional requirements)\n",
        "X = prepare_features(X, use_smi=False)\n",
        "\n",
        "# Evaluate model metrics\n",
        "y_pred = (df['prediction_0'] >= 0.5).astype(int)\n",
        "y_pred_proba = df['prediction_0']\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "auroc = roc_auc_score(y_true, y_pred_proba)\n",
        "aupr = average_precision_score(y_true, y_pred_proba)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(f'AUROC: {auroc:.2f}')\n",
        "print(f'AUPR: {aupr:.2f}')\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# compare with https://github.com/Yichuan0712/11785-TCR/blob/b1a82e928d5e753c746f82e3d552bd7e1ed9a4ff/colab/11785_project_runner.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sn5ZxFPNDjq",
        "outputId": "b4a527c1-da30-4a8a-af97-96d0b8f690a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from rdkit-pypi) (2.1.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/site-packages (from rdkit-pypi) (10.3.0)\n",
            "Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2022.9.5\n",
            "Accuracy: 0.64\n",
            "AUROC: 0.70\n",
            "AUPR: 0.68\n",
            "F1 Score: 0.65\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.63      0.64      2335\n",
            "           1       0.64      0.66      0.65      2383\n",
            "\n",
            "    accuracy                           0.64      4718\n",
            "   macro avg       0.64      0.64      0.64      4718\n",
            "weighted avg       0.64      0.64      0.64      4718\n",
            "\n"
          ]
        }
      ]
    }
  ]
}